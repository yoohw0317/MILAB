{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtzQKis/TO03++kGNMy7uk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoohw0317/MILAB/blob/main/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multivariate Linear Regression**\n",
        "\n",
        "여러개의 정보를 통해서 하나의 결과를 도출해내는 학습법\n",
        "\n",
        "\n",
        "\n",
        "matmul()로 한번에 계산\n",
        "더 간결하고, x의의 길이가 바뀌어도 코드를 바꿀 필요가 없고 속도도 더 빠름.\n",
        "\n",
        "cost function의 수식은 비슷함.\n",
        "\n",
        "W : = W -a * d/dx a\n"
      ],
      "metadata": {
        "id": "4WdMJo38Dsyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMpIsnGXDqzs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 (여기가 simple regression과 달라짐)\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "# 모델 초기화 (여기도 약간 달라짐)\n",
        "W = torch.zeros((3,1), requires_grad = True)\n",
        "b = torch.zeros(1, requires_grad = True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=1e-5)"
      ],
      "metadata": {
        "id": "4OLKLUj0nQZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "  #H(x)를 계산)\n",
        "  hypothesis = x_train.matmul(W) + b\n",
        "\n",
        "  #Cost 계산, Cost는 점점 작아지도록 됨\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  #cost로 H(x)를 개선\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print(f'Epoch {epoch:4d}/{nb_epochs} hypothesis: {hypothesis.squeeze().detach()} Cost: {cost.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESVriFr9n-9O",
        "outputId": "eb74de81-fb63-4ef4-9930-43de94528248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
            "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
            "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
            "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
            "Epoch    4/20 hypothesis: tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936096\n",
            "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371063\n",
            "Epoch    6/20 hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]) Cost: 29.758249\n",
            "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805]) Cost: 10.445267\n",
            "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391237\n",
            "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493121\n",
            "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
            "Epoch   11/20 hypothesis: tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602]) Cost: 1.710552\n",
            "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651416\n",
            "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632369\n",
            "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625924\n",
            "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623420\n",
            "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622152\n",
            "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621262\n",
            "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963]) Cost: 1.620501\n",
            "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985]) Cost: 1.619757\n",
            "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) Cost: 1.619046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nn.Module 사용하기\n",
        "\n",
        "* nn.Module 을 상속해서 모델을 생성\n",
        "* nn.Linear (3, 1)\n",
        "  * 입력 차원: 3\n",
        "  * 출력 차원: 1\n",
        "* Hypothesis 계산은 forward()에서 처리됨\n",
        "* Gradient 계산은 PyTorch가 알아서 해줌 (backward() 통해서)\n",
        "\n"
      ],
      "metadata": {
        "id": "53J2edktozkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "metadata": {
        "id": "TaHY2MNapWTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F.mse_loss\n",
        "\n",
        "\n",
        "*   torch.nn.functional 에서 제공하는 loss function 사용\n",
        "*   쉽게 다른 loss와 교체 가능 (l1_loss, smooth_l1_loss 등등)\n",
        "\n"
      ],
      "metadata": {
        "id": "sOsxSHmEpqok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 (위의 코드와 변화 없음)\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "# 모델 초기화 - 매우 간결해짐\n",
        "model = MultivariateLinearRegressionModel()\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer_new = optim.SGD(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "kXyADe8-p-OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 50\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "  #H(x)를 계산 - model을 이용해서 계산하게 됨.\n",
        "  Hypothesis_new = model(x_train)\n",
        "\n",
        "  #Cost 계산, Cost는 점점 작아지도록 됨\n",
        "  cost_new = F.mse_loss(Hypothesis_new, y_train)\n",
        "\n",
        "  #cost로 H(x)를 개선\n",
        "  optimizer_new.zero_grad()\n",
        "  cost_new.backward()\n",
        "  optimizer_new.step()\n",
        "\n",
        "  print(f'Epoch {epoch:4d}/{nb_epochs} hypothesis: {Hypothesis_new.squeeze().detach()} Cost: {cost_new.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dzekUj2qgG5",
        "outputId": "91f47da6-4eac-47cd-e3e1-916a8715c815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/50 hypothesis: tensor([4.8076, 9.0994, 7.2267, 7.6482, 8.0502]) Cost: 27175.240234\n",
            "Epoch    1/50 hypothesis: tensor([69.1865, 86.4780, 83.4692, 90.6744, 67.0705]) Cost: 8518.154297\n",
            "Epoch    2/50 hypothesis: tensor([105.2300, 129.7993, 126.1546, 137.1577, 100.1138]) Cost: 2670.148926\n",
            "Epoch    3/50 hypothesis: tensor([125.4095, 154.0533, 150.0526, 163.1820, 118.6135]) Cost: 837.110168\n",
            "Epoch    4/50 hypothesis: tensor([136.7072, 167.6323, 163.4322, 177.7520, 128.9708]) Cost: 262.549255\n",
            "Epoch    5/50 hypothesis: tensor([143.0325, 175.2346, 170.9230, 185.9093, 134.7694]) Cost: 82.455276\n",
            "Epoch    6/50 hypothesis: tensor([146.5738, 179.4908, 175.1168, 190.4762, 138.0158]) Cost: 26.005285\n",
            "Epoch    7/50 hypothesis: tensor([148.5565, 181.8737, 177.4648, 193.0331, 139.8333]) Cost: 8.311244\n",
            "Epoch    8/50 hypothesis: tensor([149.6666, 183.2077, 178.7794, 194.4646, 140.8508]) Cost: 2.765025\n",
            "Epoch    9/50 hypothesis: tensor([150.2881, 183.9546, 179.5154, 195.2660, 141.4204]) Cost: 1.026589\n",
            "Epoch   10/50 hypothesis: tensor([150.6361, 184.3727, 179.9274, 195.7148, 141.7393]) Cost: 0.481639\n",
            "Epoch   11/50 hypothesis: tensor([150.8310, 184.6068, 180.1581, 195.9660, 141.9178]) Cost: 0.310833\n",
            "Epoch   12/50 hypothesis: tensor([150.9402, 184.7378, 180.2873, 196.1066, 142.0178]) Cost: 0.257253\n",
            "Epoch   13/50 hypothesis: tensor([151.0013, 184.8111, 180.3596, 196.1854, 142.0736]) Cost: 0.240451\n",
            "Epoch   14/50 hypothesis: tensor([151.0356, 184.8521, 180.4002, 196.2295, 142.1049]) Cost: 0.235158\n",
            "Epoch   15/50 hypothesis: tensor([151.0548, 184.8751, 180.4229, 196.2542, 142.1224]) Cost: 0.233479\n",
            "Epoch   16/50 hypothesis: tensor([151.0656, 184.8879, 180.4356, 196.2680, 142.1321]) Cost: 0.232931\n",
            "Epoch   17/50 hypothesis: tensor([151.0717, 184.8951, 180.4427, 196.2757, 142.1375]) Cost: 0.232741\n",
            "Epoch   18/50 hypothesis: tensor([151.0751, 184.8990, 180.4467, 196.2801, 142.1406]) Cost: 0.232668\n",
            "Epoch   19/50 hypothesis: tensor([151.0771, 184.9012, 180.4490, 196.2825, 142.1422]) Cost: 0.232617\n",
            "Epoch   20/50 hypothesis: tensor([151.0783, 184.9024, 180.4502, 196.2839, 142.1431]) Cost: 0.232579\n",
            "Epoch   21/50 hypothesis: tensor([151.0789, 184.9031, 180.4510, 196.2847, 142.1436]) Cost: 0.232551\n",
            "Epoch   22/50 hypothesis: tensor([151.0794, 184.9034, 180.4514, 196.2851, 142.1438]) Cost: 0.232525\n",
            "Epoch   23/50 hypothesis: tensor([151.0796, 184.9035, 180.4516, 196.2854, 142.1439]) Cost: 0.232494\n",
            "Epoch   24/50 hypothesis: tensor([151.0798, 184.9036, 180.4518, 196.2855, 142.1439]) Cost: 0.232465\n",
            "Epoch   25/50 hypothesis: tensor([151.0800, 184.9036, 180.4519, 196.2856, 142.1439]) Cost: 0.232438\n",
            "Epoch   26/50 hypothesis: tensor([151.0801, 184.9036, 180.4520, 196.2857, 142.1438]) Cost: 0.232413\n",
            "Epoch   27/50 hypothesis: tensor([151.0802, 184.9036, 180.4520, 196.2857, 142.1438]) Cost: 0.232382\n",
            "Epoch   28/50 hypothesis: tensor([151.0803, 184.9035, 180.4520, 196.2857, 142.1437]) Cost: 0.232350\n",
            "Epoch   29/50 hypothesis: tensor([151.0804, 184.9035, 180.4521, 196.2857, 142.1436]) Cost: 0.232321\n",
            "Epoch   30/50 hypothesis: tensor([151.0805, 184.9034, 180.4521, 196.2858, 142.1436]) Cost: 0.232293\n",
            "Epoch   31/50 hypothesis: tensor([151.0806, 184.9033, 180.4521, 196.2858, 142.1435]) Cost: 0.232266\n",
            "Epoch   32/50 hypothesis: tensor([151.0807, 184.9033, 180.4522, 196.2858, 142.1434]) Cost: 0.232237\n",
            "Epoch   33/50 hypothesis: tensor([151.0808, 184.9032, 180.4522, 196.2858, 142.1434]) Cost: 0.232209\n",
            "Epoch   34/50 hypothesis: tensor([151.0809, 184.9032, 180.4522, 196.2858, 142.1433]) Cost: 0.232180\n",
            "Epoch   35/50 hypothesis: tensor([151.0810, 184.9031, 180.4523, 196.2858, 142.1432]) Cost: 0.232152\n",
            "Epoch   36/50 hypothesis: tensor([151.0810, 184.9030, 180.4523, 196.2858, 142.1431]) Cost: 0.232128\n",
            "Epoch   37/50 hypothesis: tensor([151.0811, 184.9030, 180.4523, 196.2858, 142.1431]) Cost: 0.232093\n",
            "Epoch   38/50 hypothesis: tensor([151.0812, 184.9029, 180.4523, 196.2858, 142.1430]) Cost: 0.232064\n",
            "Epoch   39/50 hypothesis: tensor([151.0813, 184.9028, 180.4524, 196.2859, 142.1429]) Cost: 0.232038\n",
            "Epoch   40/50 hypothesis: tensor([151.0814, 184.9028, 180.4524, 196.2859, 142.1428]) Cost: 0.232007\n",
            "Epoch   41/50 hypothesis: tensor([151.0815, 184.9027, 180.4524, 196.2859, 142.1428]) Cost: 0.231984\n",
            "Epoch   42/50 hypothesis: tensor([151.0816, 184.9027, 180.4525, 196.2859, 142.1427]) Cost: 0.231954\n",
            "Epoch   43/50 hypothesis: tensor([151.0817, 184.9026, 180.4525, 196.2859, 142.1426]) Cost: 0.231925\n",
            "Epoch   44/50 hypothesis: tensor([151.0818, 184.9025, 180.4525, 196.2859, 142.1425]) Cost: 0.231898\n",
            "Epoch   45/50 hypothesis: tensor([151.0819, 184.9025, 180.4525, 196.2859, 142.1425]) Cost: 0.231869\n",
            "Epoch   46/50 hypothesis: tensor([151.0820, 184.9024, 180.4526, 196.2860, 142.1424]) Cost: 0.231838\n",
            "Epoch   47/50 hypothesis: tensor([151.0820, 184.9024, 180.4526, 196.2860, 142.1423]) Cost: 0.231810\n",
            "Epoch   48/50 hypothesis: tensor([151.0822, 184.9023, 180.4526, 196.2860, 142.1423]) Cost: 0.231780\n",
            "Epoch   49/50 hypothesis: tensor([151.0822, 184.9022, 180.4527, 196.2860, 142.1422]) Cost: 0.231752\n",
            "Epoch   50/50 hypothesis: tensor([151.0823, 184.9022, 180.4527, 196.2860, 142.1421]) Cost: 0.231724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data in the Real World: Problem\n",
        "\n",
        "*   엄청난 양의 데이터를 한 번에 학습시키기는 불가능, 느리고 하드웨어적으로 불가능\n",
        "*   일부분의 데이터로만 학습시키는 방법\n",
        "\n",
        "# Minibatch Gradient Descent\n",
        "*   전체 데이터를 균일하게 나눠서 학습하는 방법"
      ],
      "metadata": {
        "id": "Yyl70Ylg9BRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.utils.data.Dataset 상속\n",
        "from torch.utils.data import Dataset \n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = [[73, 80, 75],\n",
        "                   [93, 88, 93],\n",
        "                   [89, 91, 90],\n",
        "                   [96, 98, 100],\n",
        "                   [73, 66, 70]]\n",
        "    self.y_data = [[152], [185], [180], [196], [142]]\n",
        "\n",
        "  # 이 데이터셋의 총 데이터 수\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "  \n",
        "  #어떠한 인덱스 idx를 입력받았을 때, 그에 상응하는ㄷ 입출력 데이터 반환\n",
        "  def __getitem__ (self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "    return x, y\n",
        "\n",
        "dataset = CustomDataset()"
      ],
      "metadata": {
        "id": "mI1BM1GA9Dox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    # 각 minibatch의 크기를 의미, 통상적으로 2의 제곱수로 설정한다.\n",
        "    batch_size=2,\n",
        "    # Epoch 마다 데이터 셋을 섞어서 데이터가 학습되는 순서를 바꾼다.\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "vqU5OvDt9HuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    for batch_idx, samples in enumerate(dataloader):\n",
        "        x_train, y_train = samples\n",
        "        \n",
        "        prediction = model(x_train)\n",
        "        cost = F.mse_loss(prediction, y_train)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        print(f'Epoch{epoch:4d}/{nb_epochs} Batch{batch_idx+1}/{len(dataloader)} Cost: {cost.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5piOuVk9JJL",
        "outputId": "ea8ac6d2-0ebc-4d4d-9d8c-92d198081f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0/20 Batch1/3 Cost: 0.107265\n",
            "Epoch   0/20 Batch2/3 Cost: 0.431069\n",
            "Epoch   0/20 Batch3/3 Cost: 0.081811\n",
            "Epoch   1/20 Batch1/3 Cost: 0.431069\n",
            "Epoch   1/20 Batch2/3 Cost: 0.045696\n",
            "Epoch   1/20 Batch3/3 Cost: 0.204949\n",
            "Epoch   2/20 Batch1/3 Cost: 0.425774\n",
            "Epoch   2/20 Batch2/3 Cost: 0.112561\n",
            "Epoch   2/20 Batch3/3 Cost: 0.081811\n",
            "Epoch   3/20 Batch1/3 Cost: 0.045696\n",
            "Epoch   3/20 Batch2/3 Cost: 0.523458\n",
            "Epoch   3/20 Batch3/3 Cost: 0.020172\n",
            "Epoch   4/20 Batch1/3 Cost: 0.014877\n",
            "Epoch   4/20 Batch2/3 Cost: 0.461889\n",
            "Epoch   4/20 Batch3/3 Cost: 0.204949\n",
            "Epoch   5/20 Batch1/3 Cost: 0.461889\n",
            "Epoch   5/20 Batch2/3 Cost: 0.107265\n",
            "Epoch   5/20 Batch3/3 Cost: 0.020172\n",
            "Epoch   6/20 Batch1/3 Cost: 0.045696\n",
            "Epoch   6/20 Batch2/3 Cost: 0.431069\n",
            "Epoch   6/20 Batch3/3 Cost: 0.204949\n",
            "Epoch   7/20 Batch1/3 Cost: 0.523458\n",
            "Epoch   7/20 Batch2/3 Cost: 0.050992\n",
            "Epoch   7/20 Batch3/3 Cost: 0.009581\n",
            "Epoch   8/20 Batch1/3 Cost: 0.107265\n",
            "Epoch   8/20 Batch2/3 Cost: 0.431069\n",
            "Epoch   8/20 Batch3/3 Cost: 0.081811\n",
            "Epoch   9/20 Batch1/3 Cost: 0.425774\n",
            "Epoch   9/20 Batch2/3 Cost: 0.143380\n",
            "Epoch   9/20 Batch3/3 Cost: 0.020172\n",
            "Epoch  10/20 Batch1/3 Cost: 0.107265\n",
            "Epoch  10/20 Batch2/3 Cost: 0.461889\n",
            "Epoch  10/20 Batch3/3 Cost: 0.020172\n",
            "Epoch  11/20 Batch1/3 Cost: 0.107265\n",
            "Epoch  11/20 Batch2/3 Cost: 0.431069\n",
            "Epoch  11/20 Batch3/3 Cost: 0.081811\n",
            "Epoch  12/20 Batch1/3 Cost: 0.431069\n",
            "Epoch  12/20 Batch2/3 Cost: 0.045696\n",
            "Epoch  12/20 Batch3/3 Cost: 0.204949\n",
            "Epoch  13/20 Batch1/3 Cost: 0.045696\n",
            "Epoch  13/20 Batch2/3 Cost: 0.523458\n",
            "Epoch  13/20 Batch3/3 Cost: 0.020172\n",
            "Epoch  14/20 Batch1/3 Cost: 0.014877\n",
            "Epoch  14/20 Batch2/3 Cost: 0.523458\n",
            "Epoch  14/20 Batch3/3 Cost: 0.081811\n",
            "Epoch  15/20 Batch1/3 Cost: 0.045696\n",
            "Epoch  15/20 Batch2/3 Cost: 0.112561\n",
            "Epoch  15/20 Batch3/3 Cost: 0.841966\n",
            "Epoch  16/20 Batch1/3 Cost: 0.425774\n",
            "Epoch  16/20 Batch2/3 Cost: 0.143380\n",
            "Epoch  16/20 Batch3/3 Cost: 0.020172\n",
            "Epoch  17/20 Batch1/3 Cost: 0.014877\n",
            "Epoch  17/20 Batch2/3 Cost: 0.523458\n",
            "Epoch  17/20 Batch3/3 Cost: 0.081811\n",
            "Epoch  18/20 Batch1/3 Cost: 0.461889\n",
            "Epoch  18/20 Batch2/3 Cost: 0.112561\n",
            "Epoch  18/20 Batch3/3 Cost: 0.009581\n",
            "Epoch  19/20 Batch1/3 Cost: 0.107265\n",
            "Epoch  19/20 Batch2/3 Cost: 0.431069\n",
            "Epoch  19/20 Batch3/3 Cost: 0.081811\n",
            "Epoch  20/20 Batch1/3 Cost: 0.014877\n",
            "Epoch  20/20 Batch2/3 Cost: 0.461889\n",
            "Epoch  20/20 Batch3/3 Cost: 0.204949\n"
          ]
        }
      ]
    }
  ]
}